{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e76c43e",
   "metadata": {},
   "source": [
    "\n",
    "# Лого Т‑банка — детекция (OWLv2 + GroundingDINO + ансамбли) ✅\n",
    "\n",
    "В этом ноутбуке:\n",
    "- аккуратные утилиты для чтения **YOLO**‑разметки и рисования боксов;\n",
    "- единая функция `evaluate_on_dataset()` с метриками **Precision / Recall / F1 / mAP@50 / mAP@50‑95**;\n",
    "- предикторы **OWLv2** и **Grounding DINO** (базовый + режимы *precise / recall / compromise*);\n",
    "- ансамбли **consensus / union / WBF**;\n",
    "- сохранение превью с боксами в подпапки `ans_*` (GT — зелёный, предикты — красный).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abfaf89f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T16:53:11.103525Z",
     "iopub.status.busy": "2025-09-17T16:53:11.103525Z",
     "iopub.status.idle": "2025-09-17T16:53:11.107272Z",
     "shell.execute_reply": "2025-09-17T16:53:11.107272Z",
     "shell.execute_reply.started": "2025-09-17T16:53:11.103525Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==== CONFIG (пути и пороги) ====\n",
    "\n",
    "# Пути к данным \n",
    "PATH_IMAGES = r\"datasets/sirius/T-bank_val/val/images\"\n",
    "PATH_LABELS = r\"datasets/sirius/T-bank_val/val/labels\"\n",
    "\n",
    "# Единственный класс (щит с буквой T)\n",
    "GT_CLASS_ID = 0\n",
    "\n",
    "# Общие пороги\n",
    "CONF = 0.30   # Порог для OWLv2\n",
    "IOU  = 0.50\n",
    "\n",
    "# Grounding DINO — базовые пороги\n",
    "BOX_THR_G  = 0.30\n",
    "TEXT_THR_G = 0.25\n",
    "\n",
    "# Промпты\n",
    "PROMPTS_OWL = [\n",
    "    \"a stylized geometric letter t inside a shield emblem\",\n",
    "    \"a minimalist t logo inside a angular shield shape\",\n",
    "    \"a modern t letter inside a geometric shield outline\",\n",
    "    \"a T-Bank logo with letter t in shield\",\n",
    "    \"t letter in white shield\"\n",
    "]\n",
    "\n",
    "PROMPTS_GDINO = (\n",
    "    \"logo with a bold black letter T inside a white shield-like shape\",\n",
    "    \"white or yellowshield emblem with a large black T in the center\",\n",
    "    \"white badge shaped like a shield with a black capital T\",\n",
    "    \"minimalist logo with a bold T inside a shield icon\",\n",
    "    \"flat design emblem shaped like a shield containing a yellow or black or white T letter\",\n",
    "    \"logo featuring a strong black T on a yellow or white shield background\",\n",
    "    \"simplified yellow or black or white shield logo with a single bold yellow or black or white letter T\"\n",
    ")\n",
    "\n",
    "# Варианты запуска\n",
    "RUN_OWLV2_COLOR = True      # базовый OWLv2\n",
    "RUN_GDINO_BASE  = True      # GDINO базовый\n",
    "RUN_GDINO_PRECISE    = True # GDINO: точный\n",
    "RUN_GDINO_RECALL     = True # GDINO: жадный\n",
    "RUN_GDINO_COMPROMISE = True # GDINO: компромисс\n",
    "RUN_ENSEMBLE_CONSENSUS = True\n",
    "RUN_ENSEMBLE_UNION     = True\n",
    "RUN_ENSEMBLE_WBF       = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "191a9fee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T16:53:11.108282Z",
     "iopub.status.busy": "2025-09-17T16:53:11.108282Z",
     "iopub.status.idle": "2025-09-17T16:53:16.299615Z",
     "shell.execute_reply": "2025-09-17T16:53:16.299615Z",
     "shell.execute_reply.started": "2025-09-17T16:53:11.108282Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrsli\\anaconda3\\envs\\work_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== IMPORTS ====\n",
    "import os, glob, torch, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Callable\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torchvision.ops import box_iou, nms\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "from transformers import AutoProcessor, Owlv2ForObjectDetection\n",
    "from transformers import AutoModelForZeroShotObjectDetection as GDINOModel, AutoProcessor as GDINOProcessor\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9237b25f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T16:53:16.300624Z",
     "iopub.status.busy": "2025-09-17T16:53:16.299615Z",
     "iopub.status.idle": "2025-09-17T16:53:16.312687Z",
     "shell.execute_reply": "2025-09-17T16:53:16.312687Z",
     "shell.execute_reply.started": "2025-09-17T16:53:16.300624Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==== UTILITIES ====\n",
    "\n",
    "# Поддерживаемые расширения картинок\n",
    "SUPPORTED_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "\n",
    "\n",
    "def gpu_cleanup(sync: bool = False):\n",
    "    \"\"\"Чистим кеш CUDA\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        if sync:\n",
    "            torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "        # иногда полезно, но не у всех сборок CUDA есть:\n",
    "        try:\n",
    "            torch.cuda.ipc_collect()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "def list_images(root: str) -> List[str]:\n",
    "    \"\"\"Рекурсивно собираем пути к картинкам.\"\"\"\n",
    "    paths = []\n",
    "    for ext in SUPPORTED_EXTS:\n",
    "        paths.extend(glob.glob(os.path.join(root, f\"**/*{ext}\"), recursive=True))\n",
    "    return sorted(paths)\n",
    "\n",
    "    \n",
    "def yolo_path_for(img_path: str, labels_root: str) -> str:\n",
    "    \"\"\"Строим путь к .txt по имени файла изображения.\"\"\"\n",
    "    stem = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    return os.path.join(labels_root, f\"{stem}.txt\")\n",
    "\n",
    "\n",
    "def load_yolo_labels(lbl_path: str, img_wh: Tuple[int, int]):\n",
    "    \"\"\" ИЗ формата YOLO (cls cx cy w h) в формат left_x left_y right_x right_y cls\"\"\"\n",
    "    w, h = img_wh\n",
    "    if not os.path.isfile(lbl_path):\n",
    "        return np.zeros((0,4), np.float32), np.zeros((0,), np.int64)\n",
    "    boxes, labels = [], []\n",
    "    with open(lbl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s: \n",
    "                continue\n",
    "            p = s.split()\n",
    "            if len(p) != 5:\n",
    "                continue\n",
    "            cls = int(float(p[0]))\n",
    "            xc, yc, bw, bh = map(float, p[1:])\n",
    "            px, py, pw, ph = xc*w, yc*h, bw*w, bh*h\n",
    "            x1, y1 = max(0.0, px - pw/2), max(0.0, py - ph/2)\n",
    "            x2, y2 = min(w,   px + pw/2), min(h,   py + ph/2)\n",
    "            boxes.append([x1, y1, x2, y2]); labels.append(cls)\n",
    "    if not boxes:\n",
    "        return np.zeros((0,4), np.float32), np.zeros((0,), np.int64)\n",
    "    return np.asarray(boxes, np.float32), np.asarray(labels, np.int64)\n",
    "\n",
    "\n",
    "def measure_text(draw, text, font):\n",
    "    \"\"\"Безопасное измерение текста для разных версий Pillow.\"\"\"\n",
    "    try:\n",
    "        l, t, r, b = draw.textbbox((0, 0), text, font=font)\n",
    "        return (r - l), (b - t)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return font.getsize(text)\n",
    "        except Exception:\n",
    "            return (len(text) * 7, 10)\n",
    "\n",
    "\n",
    "def pr_counts_torch(pred_boxes, pred_scores, gt_boxes, iou_thr=0.5):\n",
    "    \"\"\"Счётчики TP/FP/FN для precision/recall — жадный матч по IoU.\"\"\"\n",
    "    pb = torch.as_tensor(pred_boxes, dtype=torch.float32)\n",
    "    gb = torch.as_tensor(gt_boxes, dtype=torch.float32)\n",
    "    if pb.numel() == 0 and gb.numel() == 0: return 0, 0, 0\n",
    "    if pb.numel() == 0: return 0, 0, gb.shape[0]\n",
    "    if gb.numel() == 0: return 0, pb.shape[0], 0\n",
    "    order = torch.argsort(torch.as_tensor(pred_scores), descending=True)\n",
    "    pb = pb[order]\n",
    "    ious = box_iou(pb, gb)\n",
    "    used = torch.zeros(gb.shape[0], dtype=torch.bool)\n",
    "    tp = fp = 0\n",
    "    for i in range(pb.shape[0]):\n",
    "        row = ious[i].clone()\n",
    "        row[used] = -1\n",
    "        best_iou, best_j = row.max(0)\n",
    "        if best_iou >= iou_thr:\n",
    "            tp += 1; used[best_j] = True\n",
    "        else:\n",
    "            fp += 1\n",
    "    fn = int((~used).sum().item())\n",
    "    return int(tp), int(fp), fn\n",
    "\n",
    "    \n",
    "def draw_and_save(img: Image.Image, gt_boxes: np.ndarray, pred_boxes: np.ndarray, pred_scores: np.ndarray, save_path: str):\n",
    "    \"\"\"Отрисовываем bound boxes, как GT и inference\"\"\"\n",
    "    out = img.copy()\n",
    "    W, H = out.size\n",
    "    lw_pred = max(2, int(round(0.004 * 0.5 * (W + H))))  # базовая толщина\n",
    "    lw_gt   = lw_pred + 4                                # зеленые толще, чтобы не было перекрытий\n",
    "    PAD = 5                                              # расширяем GT на 5 пикселей\n",
    "\n",
    "    def clamp_box(x1, y1, x2, y2):\n",
    "        x1 = max(0, min(W - 1, int(round(x1))))\n",
    "        y1 = max(0, min(H - 1, int(round(y1))))\n",
    "        x2 = max(0, min(W - 1, int(round(x2))))\n",
    "        y2 = max(0, min(H - 1, int(round(y2))))\n",
    "        if x2 <= x1: x2 = min(W - 1, x1 + 1)\n",
    "        if y2 <= y1: y2 = min(H - 1, y1 + 1)\n",
    "        return x1, y1, x2, y2\n",
    "\n",
    "    draw = ImageDraw.Draw(out)\n",
    "    try: \n",
    "        font = ImageFont.load_default()\n",
    "    except Exception:\n",
    "        font = None\n",
    "    if font is None:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # GT boxes (зелёные)\n",
    "    for gb in gt_boxes:\n",
    "        x1, y1, x2, y2 = [float(v) for v in gb]\n",
    "        x1, y1, x2, y2 = clamp_box(x1 - PAD, y1 - PAD, x2 + PAD, y2 + PAD)\n",
    "        draw.rectangle([(x1, y1), (x2, y2)], outline=(0, 255, 0), width=lw_gt)\n",
    "        t = \"GT\"; tw, th = measure_text(draw, t, font)\n",
    "        tx, ty = x1, max(0, y1 - th - 2)\n",
    "        draw.rectangle([(tx, ty), (tx + tw + 2, ty + th + 2)], fill=(0, 255, 0))\n",
    "        draw.text((tx + 1, ty + 1), t, fill=(0, 0, 0), font=font)\n",
    "\n",
    "    # inference boxes (красные)\n",
    "    for b, s in zip(pred_boxes, pred_scores):\n",
    "        x1, y1, x2, y2 = [float(v) for v in b]\n",
    "        x1, y1, x2, y2 = clamp_box(x1, y1, x2, y2)\n",
    "        draw.rectangle([(x1, y1), (x2, y2)], outline=(255, 0, 0), width=lw_pred)\n",
    "        t = f\"{s:.2f}\"; tw, th = measure_text(draw, t, font)\n",
    "        tx, ty = x1, max(0, y1 - th - 2)\n",
    "        draw.rectangle([(tx, ty), (tx + tw + 2, ty + th + 2)], fill=(255, 255, 0))\n",
    "        draw.text((tx + 1, ty + 1), t, fill=(0, 0, 0), font=font)\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    out.save(save_path)\n",
    "    out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28286484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T16:53:16.313693Z",
     "iopub.status.busy": "2025-09-17T16:53:16.313693Z",
     "iopub.status.idle": "2025-09-17T16:53:16.323933Z",
     "shell.execute_reply": "2025-09-17T16:53:16.323933Z",
     "shell.execute_reply.started": "2025-09-17T16:53:16.313693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ==== DATASET ====\n",
    "img_paths = list_images(PATH_IMAGES)\n",
    "assert img_paths, f\"Нет изображений по пути: {PATH_IMAGES}\"\n",
    "len(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd2cc7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T16:53:16.323933Z",
     "iopub.status.busy": "2025-09-17T16:53:16.323933Z",
     "iopub.status.idle": "2025-09-17T16:53:16.330092Z",
     "shell.execute_reply": "2025-09-17T16:53:16.330092Z",
     "shell.execute_reply.started": "2025-09-17T16:53:16.323933Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==== EVALUATION (Precision / Recall / F1 / mAP) ====\n",
    "\n",
    "def evaluate_on_dataset(predict_fn: Callable, run_name: str, out_dir: str, save_images: bool=True):\n",
    "    \"\"\"Предсказываем, а после считаем метрики + сохраняем визуализацию\"\"\"\n",
    "    map_metric = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "    TP = FP = FN = 0\n",
    "\n",
    "    for img_path in tqdm(img_paths, desc=run_name, unit=\"img\"):\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            continue\n",
    "        W, H = img.size\n",
    "\n",
    "        # GT (берём только целевой класс)\n",
    "        gt_boxes, gt_labels = load_yolo_labels(yolo_path_for(img_path, PATH_LABELS), (W, H))\n",
    "        keep = (gt_labels == GT_CLASS_ID)\n",
    "        gt_boxes = gt_boxes[keep]; gt_labels = gt_labels[keep]\n",
    "\n",
    "        # Предсказание\n",
    "        boxes, scores = predict_fn(img)\n",
    "        labels = np.zeros((boxes.shape[0],), dtype=np.int64)  # один класс\n",
    "\n",
    "        # mAP (torchmetrics ждёт тензоры)\n",
    "        map_metric.update(\n",
    "            [{ \"boxes\": torch.from_numpy(boxes),\n",
    "               \"scores\": torch.from_numpy(scores),\n",
    "               \"labels\": torch.from_numpy(labels)}],\n",
    "            [{ \"boxes\": torch.from_numpy(gt_boxes),\n",
    "               \"labels\": torch.from_numpy(gt_labels)}]\n",
    "        )\n",
    "\n",
    "        # Precision/Recall счётчики\n",
    "        tp, fp, fn = pr_counts_torch(boxes, scores, gt_boxes, IOU)\n",
    "        TP += tp; FP += fp; FN += fn\n",
    "\n",
    "        # Визуализация\n",
    "        if save_images:\n",
    "            stem, ext = os.path.splitext(os.path.basename(img_path))\n",
    "            save_path = os.path.join(out_dir, f\"{run_name.lower()}_{stem}{ext}\")\n",
    "            draw_and_save(img, gt_boxes, boxes, scores, save_path)\n",
    "\n",
    "        img.close()\n",
    "\n",
    "    # Сводка\n",
    "    res_map = map_metric.compute()\n",
    "    precision = TP / max(1, TP + FP)\n",
    "    recall    = TP / max(1, TP + FN)\n",
    "    f1        = 0.0 if (precision + recall) == 0 else (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    summary = {\n",
    "        \"run\": run_name,\n",
    "        \"images\": len(img_paths),\n",
    "        \"conf\": CONF,\n",
    "        \"iou_eval\": IOU,\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"mAP@50\": float(res_map[\"map_50\"]),\n",
    "        \"mAP@50-95\": float(res_map[\"map\"]),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n===== {run_name} =====\")\n",
    "    print(f\"Images: {summary['images']}  Conf>= {CONF}  IoU>= {IOU}\")\n",
    "    print(f\"Precision:  {summary['precision']:.4f}\")\n",
    "    print(f\"Recall:     {summary['recall']:.4f}\")\n",
    "    print(f\"F1@{IOU}:   {summary['f1']:.4f}\")\n",
    "    print(f\"mAP@50:     {summary['mAP@50']:.4f}\")\n",
    "    print(f\"mAP@50-95:  {summary['mAP@50-95']:.4f}\")\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a344c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T16:53:16.331098Z",
     "iopub.status.busy": "2025-09-17T16:53:16.330092Z",
     "iopub.status.idle": "2025-09-17T16:53:20.322901Z",
     "shell.execute_reply": "2025-09-17T16:53:20.322901Z",
     "shell.execute_reply.started": "2025-09-17T16:53:16.331098Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== OWLv2 MODEL ====\n",
    "processor_owl = AutoProcessor.from_pretrained(\"google/owlv2-large-patch14-ensemble\")\n",
    "model_owl = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-large-patch14-ensemble\").to(device).eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def owlv2_predict_color(img: Image.Image):\n",
    "    W, H = img.size\n",
    "    inputs = processor_owl(text=[PROMPTS_OWL], images=img, return_tensors=\"pt\").to(device)\n",
    "    outputs = model_owl(**inputs)\n",
    "    res = processor_owl.post_process_object_detection(\n",
    "        outputs=outputs, threshold=CONF, target_sizes=torch.tensor([(H, W)], device=device)\n",
    "    )[0]\n",
    "    boxes  = res[\"boxes\"].detach().cpu().numpy().astype(np.float32)\n",
    "    scores = res[\"scores\"].detach().cpu().numpy().astype(np.float32)\n",
    "    return boxes, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "421b9df4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T16:53:20.323905Z",
     "iopub.status.busy": "2025-09-17T16:53:20.323905Z",
     "iopub.status.idle": "2025-09-17T16:53:24.068063Z",
     "shell.execute_reply": "2025-09-17T16:53:24.068063Z",
     "shell.execute_reply.started": "2025-09-17T16:53:20.323905Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "Fetching 1 files: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== Grounding DINO MODEL (база) ====\n",
    "gdino_model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "processor_g = GDINOProcessor.from_pretrained(gdino_model_id)\n",
    "model_g     = GDINOModel.from_pretrained(gdino_model_id).to(device).eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def gdino_predict(img: Image.Image):\n",
    "    \"\"\"Базовый предиктор GDINO c порогами BOX_THR_G / TEXT_THR_G.\"\"\"\n",
    "    W, H = img.size\n",
    "    inputs = processor_g(images=img, text=[PROMPTS_GDINO], return_tensors=\"pt\").to(device)\n",
    "    outputs = model_g(**inputs)\n",
    "    res = processor_g.post_process_grounded_object_detection(\n",
    "        outputs=outputs, input_ids=inputs.input_ids,\n",
    "        threshold=BOX_THR_G, text_threshold=TEXT_THR_G, target_sizes=[(H, W)]\n",
    "    )[0]\n",
    "    boxes  = res.get(\"boxes\", torch.empty((0,4))).detach().cpu().numpy().astype(np.float32)\n",
    "    scores = res.get(\"scores\", torch.empty((0,))).detach().cpu().numpy().astype(np.float32)\n",
    "    return boxes, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b00ab0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T16:53:24.069068Z",
     "iopub.status.busy": "2025-09-17T16:53:24.068063Z",
     "iopub.status.idle": "2025-09-17T16:53:24.079550Z",
     "shell.execute_reply": "2025-09-17T16:53:24.079550Z",
     "shell.execute_reply.started": "2025-09-17T16:53:24.069068Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==== Grounding DINO — расширенные режимы (precise / recall / compromise) ====\n",
    "# Идея:\n",
    "#  - precise: выше пороги + строгий shape-фильтр + NMS → ↑ precision\n",
    "#  - recall:  мягкие пороги (2 прохода, union) + мягкий shape-фильтр + NMS → ↑ recall\n",
    "#  - compromise: объединение (union) precise и recall → затем NMS для баланса\n",
    "\n",
    "\n",
    "# Пороговые параметры\n",
    "BOX_THR_G_P1, TEXT_THR_G_P1 = 0.35, 0.25  # precise, 1-й проход\n",
    "BOX_THR_G_P2, TEXT_THR_G_P2 = 0.30, 0.22  # precise, fallback\n",
    "\n",
    "BOX_THR_G_R1, TEXT_THR_G_R1 = 0.30, 0.25  # recall, 1-й проход\n",
    "BOX_THR_G_R2, TEXT_THR_G_R2 = 0.25, 0.20  # recall, 2-й (ещё мягче)\n",
    "\n",
    "# Shape-фильтры (соотношение сторон и доля площади от изображения)\n",
    "ASPECT_MIN_P, ASPECT_MAX_P   = 0.75, 1.35\n",
    "AREA_MIN_FR_P, AREA_MAX_FR_P = 0.00025, 0.14\n",
    "\n",
    "ASPECT_MIN_R, ASPECT_MAX_R   = 0.65, 1.45\n",
    "AREA_MIN_FR_R, AREA_MAX_FR_R = 0.00015, 0.20\n",
    "\n",
    "# NMS и ограничение числа детектов\n",
    "NMS_IOU_PRE, MAX_DETS_PRE = 0.55, 50\n",
    "NMS_IOU_REC, MAX_DETS_REC = 0.60, 80\n",
    "\n",
    "def _gdino_pass(img, box_thr, text_thr):\n",
    "    W, H = img.size\n",
    "    inputs = processor_g(images=img, text=[PROMPTS_GDINO], return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model_g(**inputs)\n",
    "    res = processor_g.post_process_grounded_object_detection(\n",
    "        outputs=outputs, input_ids=inputs.input_ids,\n",
    "        threshold=box_thr, text_threshold=text_thr, target_sizes=[(H, W)]\n",
    "    )[0]\n",
    "    boxes  = res.get(\"boxes\", torch.empty((0,4))).detach().cpu().numpy().astype(np.float32)\n",
    "    scores = res.get(\"scores\", torch.empty((0,))).detach().cpu().numpy().astype(np.float32)\n",
    "    return boxes, scores\n",
    "\n",
    "def _shape_filter(boxes, scores, W, H, a_min, a_max, fr_min, fr_max):\n",
    "    if len(boxes) == 0: return boxes, scores\n",
    "    img_area = float(W * H)\n",
    "    keep = []\n",
    "    for i, b in enumerate(boxes):\n",
    "        x1, y1, x2, y2 = map(float, b)\n",
    "        w = max(1.0, x2 - x1); h = max(1.0, y2 - y1)\n",
    "        ar = w / h\n",
    "        fr = (w * h) / img_area\n",
    "        if a_min <= ar <= a_max and fr_min <= fr <= fr_max:\n",
    "            keep.append(i)\n",
    "    if not keep:\n",
    "        return np.zeros((0,4), np.float32), np.zeros((0,), np.float32)\n",
    "    keep = np.asarray(keep, int)\n",
    "    return boxes[keep], scores[keep]\n",
    "\n",
    "@torch.inference_mode()\n",
    "def gdino_predict_precise(img: Image.Image):\n",
    "    W, H = img.size\n",
    "    b1, s1 = _gdino_pass(img, BOX_THR_G_P1, TEXT_THR_G_P1)\n",
    "    if len(b1) == 0:\n",
    "        b2, s2 = _gdino_pass(img, BOX_THR_G_P2, TEXT_THR_G_P2)\n",
    "        b, s = (b2, s2)\n",
    "    else:\n",
    "        b, s = (b1, s1)\n",
    "    b, s = _shape_filter(b, s, W, H, ASPECT_MIN_P, ASPECT_MAX_P, AREA_MIN_FR_P, AREA_MAX_FR_P)\n",
    "    if len(b):\n",
    "        keep = nms(torch.from_numpy(b), torch.from_numpy(s), iou_threshold=NMS_IOU_PRE).numpy()\n",
    "        b, s = b[keep], s[keep]\n",
    "        if len(b) > MAX_DETS_PRE:\n",
    "            order = np.argsort(-s)[:MAX_DETS_PRE]\n",
    "            b, s = b[order], s[order]\n",
    "    return b, s\n",
    "\n",
    "@torch.inference_mode()\n",
    "def gdino_predict_recall(img: Image.Image):\n",
    "    W, H = img.size\n",
    "    b1, s1 = _gdino_pass(img, BOX_THR_G_R1, TEXT_THR_G_R1)\n",
    "    b2, s2 = _gdino_pass(img, BOX_THR_G_R2, TEXT_THR_G_R2)\n",
    "    if len(b1) == 0 and len(b2) == 0:\n",
    "        B, S = b1, s1\n",
    "    elif len(b1) == 0:\n",
    "        B, S = b2, s2\n",
    "    elif len(b2) == 0:\n",
    "        B, S = b1, s1\n",
    "    else:\n",
    "        B, S = np.vstack([b1, b2]), np.hstack([s1, s2])\n",
    "    B, S = _shape_filter(B, S, W, H, ASPECT_MIN_R, ASPECT_MAX_R, AREA_MIN_FR_R, AREA_MAX_FR_R)\n",
    "    if len(B):\n",
    "        keep = nms(torch.from_numpy(B), torch.from_numpy(S), iou_threshold=NMS_IOU_REC).numpy()\n",
    "        B, S = B[keep], S[keep]\n",
    "        if len(B) > MAX_DETS_REC:\n",
    "            order = np.argsort(-S)[:MAX_DETS_REC]\n",
    "            B, S = B[order], S[order]\n",
    "    return B, S\n",
    "\n",
    "@torch.inference_mode()\n",
    "def gdino_predict_compromise(img: Image.Image):\n",
    "    b_prec, s_prec = gdino_predict_precise(img)\n",
    "    b_rec,  s_rec  = gdino_predict_recall(img)\n",
    "    if len(b_prec)==0 and len(b_rec)==0:\n",
    "        return b_prec, s_prec\n",
    "    if len(b_prec)==0:\n",
    "        B, S = b_rec, s_rec\n",
    "    elif len(b_rec)==0:\n",
    "        B, S = b_prec, s_prec\n",
    "    else:\n",
    "        B, S = np.vstack([b_prec, b_rec]), np.hstack([s_prec, s_rec])\n",
    "    if len(B):\n",
    "        keep = nms(torch.from_numpy(B), torch.from_numpy(S), iou_threshold=0.55).numpy()\n",
    "        B, S = B[keep], S[keep]\n",
    "    return B, S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c68522b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T16:53:24.080557Z",
     "iopub.status.busy": "2025-09-17T16:53:24.079550Z",
     "iopub.status.idle": "2025-09-17T16:53:24.090743Z",
     "shell.execute_reply": "2025-09-17T16:53:24.090743Z",
     "shell.execute_reply.started": "2025-09-17T16:53:24.080557Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==== Ансамбли OWLv2 + GDINO (consensus / union / WBF) ====\n",
    "\n",
    "# Ручки ансамбля\n",
    "ENSEMBLE_IOU      = 0.50   # совпадение между моделями (для consensus/WBF)\n",
    "NMS_IOU_FINAL     = 0.50   # финальный NMS\n",
    "MAX_DETS_ENSEMBLE = 80\n",
    "\n",
    "# Веса для WBF (OWL немного важнее по точности)\n",
    "W_OWL, W_GDINO = 1.00, 0.85\n",
    "\n",
    "# Какой GDINO использовать в ансамбле: 'base' / 'precise' / 'recall' / 'compromise'\n",
    "ENSEMBLE_GDINO_FN = 'recall'\n",
    "\n",
    "def _get_gdino_for_ensemble():\n",
    "    if ENSEMBLE_GDINO_FN == 'precise': return gdino_predict_precise\n",
    "    if ENSEMBLE_GDINO_FN == 'recall':  return gdino_predict_recall\n",
    "    if ENSEMBLE_GDINO_FN == 'compromise': return gdino_predict_compromise\n",
    "    return gdino_predict  # base\n",
    "\n",
    "def _final_nms(B, S):\n",
    "    if len(B)==0: return B, S\n",
    "    keep = nms(torch.from_numpy(B), torch.from_numpy(S), iou_threshold=NMS_IOU_FINAL).numpy()\n",
    "    B, S = B[keep], S[keep]\n",
    "    if len(B) > MAX_DETS_ENSEMBLE:\n",
    "        order = np.argsort(-S)[:MAX_DETS_ENSEMBLE]\n",
    "        B, S = B[order], S[order]\n",
    "    return B, S\n",
    "\n",
    "def _consensus_ensemble(img: Image.Image):\n",
    "    \"\"\"OWL как база, берём только те OWL-боксы, что подтверждены GDINO (IoU≥ENSEMBLE_IOU).\"\"\"\n",
    "    b_owl, s_owl = owlv2_predict_color(img)\n",
    "    b_g,   s_g   = _get_gdino_for_ensemble()(img)\n",
    "    if len(b_owl)==0 or len(b_g)==0:\n",
    "        return np.zeros((0,4), np.float32), np.zeros((0,), np.float32)\n",
    "    ious = box_iou(torch.tensor(b_owl), torch.tensor(b_g)).numpy()\n",
    "    keep_idx, keep_scores = [], []\n",
    "    for i in range(ious.shape[0]):\n",
    "        j = int(ious[i].argmax())\n",
    "        if ious[i, j] >= ENSEMBLE_IOU:\n",
    "            keep_idx.append(i)\n",
    "            keep_scores.append(float(max(s_owl[i], s_g[j])))\n",
    "    if not keep_idx:\n",
    "        return np.zeros((0,4), np.float32), np.zeros((0,), np.float32)\n",
    "    B = b_owl[np.asarray(keep_idx, int)]\n",
    "    S = np.asarray(keep_scores, np.float32)\n",
    "    return _final_nms(B, S)\n",
    "\n",
    "def _union_ensemble(img: Image.Image):\n",
    "    \"\"\"Объединяем OWL и GDINO, затем финальный NMS.\"\"\"\n",
    "    b_owl, s_owl = owlv2_predict_color(img)\n",
    "    b_g,   s_g   = _get_gdino_for_ensemble()(img)\n",
    "    if len(b_owl)==0 and len(b_g)==0:\n",
    "        return b_owl, s_owl\n",
    "    if len(b_owl)==0:\n",
    "        return _final_nms(b_g, s_g)\n",
    "    if len(b_g)==0:\n",
    "        return _final_nms(b_owl, s_owl)\n",
    "    B = np.vstack([b_owl, b_g]); S = np.hstack([s_owl, s_g])\n",
    "    return _final_nms(B, S)\n",
    "\n",
    "def _wbf_ensemble(img: Image.Image):\n",
    "    \"\"\"Простой Weighted Boxes Fusion по IoU≥ENSEMBLE_IOU.\"\"\"\n",
    "    b_owl, s_owl = owlv2_predict_color(img)\n",
    "    b_g,   s_g   = _get_gdino_for_ensemble()(img)\n",
    "\n",
    "    if len(b_owl)==0 and len(b_g)==0:\n",
    "        return np.zeros((0,4), np.float32), np.zeros((0,), np.float32)\n",
    "\n",
    "    B = []; S = []; Wt = []\n",
    "    if len(b_owl):\n",
    "        B.append(b_owl); S.append(s_owl); Wt.append(np.full_like(s_owl, W_OWL, dtype=np.float32))\n",
    "    if len(b_g):\n",
    "        B.append(b_g);   S.append(s_g);   Wt.append(np.full_like(s_g,   W_GDINO, dtype=np.float32))\n",
    "    B = np.vstack(B)\n",
    "    S = np.hstack(S).astype(np.float32)\n",
    "    Wt = np.hstack(Wt).astype(np.float32)\n",
    "\n",
    "    clusters = []  # элементы: {'box': np.array(4), 'score': float, 'weight': float}\n",
    "    for i in np.argsort(-S):\n",
    "        bi, si, wi = B[i], S[i], Wt[i]\n",
    "        matched = False\n",
    "        for c in clusters:\n",
    "            xA = max(c['box'][0], bi[0]); yA = max(c['box'][1], bi[1])\n",
    "            xB = min(c['box'][2], bi[2]); yB = min(c['box'][3], bi[3])\n",
    "            inter = max(0.0, xB - xA) * max(0.0, yB - yA)\n",
    "            if inter <= 0: \n",
    "                continue\n",
    "            area_c = max(1.0, (c['box'][2]-c['box'][0])*(c['box'][3]-c['box'][1]))\n",
    "            area_i = max(1.0, (bi[2]-bi[0])*(bi[3]-bi[1]))\n",
    "            iou = inter / (area_c + area_i - inter)\n",
    "            if iou >= ENSEMBLE_IOU:\n",
    "                total_w  = c['weight'] + si*wi\n",
    "                c['box']   = (c['box']*c['weight'] + bi*(si*wi)) / total_w\n",
    "                c['score'] = (c['score']*c['weight'] + si*wi) / total_w\n",
    "                c['weight']= total_w\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            clusters.append({'box': bi.copy(), 'score': si*wi, 'weight': si*wi})\n",
    "\n",
    "    if not clusters:\n",
    "        return np.zeros((0,4), np.float32), np.zeros((0,), np.float32)\n",
    "\n",
    "    Bf = np.vstack([c['box'] for c in clusters]).astype(np.float32)\n",
    "    Sf = np.array([c['score']/max(1e-6, c['weight']) for c in clusters], dtype=np.float32)\n",
    "\n",
    "    return _final_nms(Bf, Sf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7559ae57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T16:53:24.091751Z",
     "iopub.status.busy": "2025-09-17T16:53:24.091751Z",
     "iopub.status.idle": "2025-09-17T17:00:20.536855Z",
     "shell.execute_reply": "2025-09-17T17:00:20.536855Z",
     "shell.execute_reply.started": "2025-09-17T16:53:24.091751Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OWLv2_COLOR:   0%|                                                                            | 0/146 [00:00<?, ?img/s]C:\\Users\\mrsli\\anaconda3\\envs\\work_env\\Lib\\site-packages\\transformers\\models\\owlv2\\processing_owlv2.py:201: FutureWarning: `post_process_object_detection` method is deprecated for OwlVitProcessor and will be removed in v5. Use `post_process_grounded_object_detection` instead.\n",
      "  warnings.warn(\n",
      "OWLv2_COLOR: 100%|██████████████████████████████████████████████████████████████████| 146/146 [02:08<00:00,  1.14img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OWLv2_COLOR =====\n",
      "Images: 146  Conf>= 0.3  IoU>= 0.5\n",
      "Precision:  0.7324\n",
      "Recall:     0.4333\n",
      "F1@0.5:   0.5445\n",
      "mAP@50:     0.3742\n",
      "mAP@50-95:  0.3443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GroundingDINO_BASE: 100%|███████████████████████████████████████████████████████████| 146/146 [00:38<00:00,  3.80img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== GroundingDINO_BASE =====\n",
      "Images: 146  Conf>= 0.3  IoU>= 0.5\n",
      "Precision:  0.1399\n",
      "Recall:     0.8917\n",
      "F1@0.5:   0.2418\n",
      "mAP@50:     0.2369\n",
      "mAP@50-95:  0.1956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GroundingDINO_PRECISE: 100%|████████████████████████████████████████████████████████| 146/146 [00:42<00:00,  3.43img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== GroundingDINO_PRECISE =====\n",
      "Images: 146  Conf>= 0.3  IoU>= 0.5\n",
      "Precision:  0.4134\n",
      "Recall:     0.6167\n",
      "F1@0.5:   0.4950\n",
      "mAP@50:     0.2918\n",
      "mAP@50-95:  0.2429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GroundingDINO_RECALL: 100%|█████████████████████████████████████████████████████████| 146/146 [01:18<00:00,  1.85img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== GroundingDINO_RECALL =====\n",
      "Images: 146  Conf>= 0.3  IoU>= 0.5\n",
      "Precision:  0.2367\n",
      "Recall:     0.8500\n",
      "F1@0.5:   0.3702\n",
      "mAP@50:     0.3533\n",
      "mAP@50-95:  0.2872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GroundingDINO_COMPROMISE: 100%|█████████████████████████████████████████████████████| 146/146 [02:06<00:00,  1.15img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== GroundingDINO_COMPROMISE =====\n",
      "Images: 146  Conf>= 0.3  IoU>= 0.5\n",
      "Precision:  0.2383\n",
      "Recall:     0.8500\n",
      "F1@0.5:   0.3723\n",
      "mAP@50:     0.3533\n",
      "mAP@50-95:  0.2872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== RUN ====\n",
    "results = []\n",
    "\n",
    "# OWLv2 базовый\n",
    "if RUN_OWLV2_COLOR:\n",
    "    out_dir = os.path.join(os.path.dirname(PATH_IMAGES), \"ans_owlv2_color\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    results.append(evaluate_on_dataset(owlv2_predict_color, \"OWLv2_COLOR\", out_dir, save_images=True))\n",
    "    gpu_cleanup()\n",
    "\n",
    "# GDINO базовый\n",
    "if RUN_GDINO_BASE:\n",
    "    out_dir = os.path.join(os.path.dirname(PATH_IMAGES), \"ans_gdino_base\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    results.append(evaluate_on_dataset(gdino_predict, \"GroundingDINO_BASE\", out_dir, save_images=True))\n",
    "    gpu_cleanup()\n",
    "\n",
    "# GDINO режимы\n",
    "if RUN_GDINO_PRECISE:\n",
    "    out_dir = os.path.join(os.path.dirname(PATH_IMAGES), \"ans_gdino_precise\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    results.append(evaluate_on_dataset(gdino_predict_precise, \"GroundingDINO_PRECISE\", out_dir, save_images=True))\n",
    "    gpu_cleanup()\n",
    "\n",
    "if RUN_GDINO_RECALL:\n",
    "    out_dir = os.path.join(os.path.dirname(PATH_IMAGES), \"ans_gdino_recall\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    results.append(evaluate_on_dataset(gdino_predict_recall, \"GroundingDINO_RECALL\", out_dir, save_images=True))\n",
    "    gpu_cleanup()\n",
    "\n",
    "if RUN_GDINO_COMPROMISE:\n",
    "    out_dir = os.path.join(os.path.dirname(PATH_IMAGES), \"ans_gdino_compromise\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    results.append(evaluate_on_dataset(gdino_predict_compromise, \"GroundingDINO_COMPROMISE\", out_dir, save_images=True))\n",
    "    gpu_cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02138a10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T17:00:20.537862Z",
     "iopub.status.busy": "2025-09-17T17:00:20.537862Z",
     "iopub.status.idle": "2025-09-17T17:10:49.353870Z",
     "shell.execute_reply": "2025-09-17T17:10:49.353870Z",
     "shell.execute_reply.started": "2025-09-17T17:00:20.537862Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ENSEMBLE_CONSENSUS: 100%|███████████████████████████████████████████████████████████| 146/146 [03:29<00:00,  1.43s/img]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ENSEMBLE_CONSENSUS =====\n",
      "Images: 146  Conf>= 0.3  IoU>= 0.5\n",
      "Precision:  0.8197\n",
      "Recall:     0.4167\n",
      "F1@0.5:   0.5525\n",
      "mAP@50:     0.3652\n",
      "mAP@50-95:  0.3367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ENSEMBLE_UNION: 100%|███████████████████████████████████████████████████████████████| 146/146 [03:29<00:00,  1.44s/img]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ENSEMBLE_UNION =====\n",
      "Images: 146  Conf>= 0.3  IoU>= 0.5\n",
      "Precision:  0.2402\n",
      "Recall:     0.8667\n",
      "F1@0.5:   0.3761\n",
      "mAP@50:     0.4038\n",
      "mAP@50-95:  0.3457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ENSEMBLE_WBF: 100%|█████████████████████████████████████████████████████████████████| 146/146 [03:29<00:00,  1.43s/img]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ENSEMBLE_WBF =====\n",
      "Images: 146  Conf>= 0.3  IoU>= 0.5\n",
      "Precision:  0.2402\n",
      "Recall:     0.8667\n",
      "F1@0.5:   0.3761\n",
      "mAP@50:     0.2686\n",
      "mAP@50-95:  0.2128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'run': 'OWLv2_COLOR',\n",
       "  'images': 146,\n",
       "  'conf': 0.3,\n",
       "  'iou_eval': 0.5,\n",
       "  'precision': 0.7323943661971831,\n",
       "  'recall': 0.43333333333333335,\n",
       "  'f1': 0.5445026178010471,\n",
       "  'mAP@50': 0.37418845295906067,\n",
       "  'mAP@50-95': 0.34431517124176025},\n",
       " {'run': 'GroundingDINO_BASE',\n",
       "  'images': 146,\n",
       "  'conf': 0.3,\n",
       "  'iou_eval': 0.5,\n",
       "  'precision': 0.13986928104575164,\n",
       "  'recall': 0.8916666666666667,\n",
       "  'f1': 0.24180790960451978,\n",
       "  'mAP@50': 0.2368956059217453,\n",
       "  'mAP@50-95': 0.19561053812503815},\n",
       " {'run': 'GroundingDINO_PRECISE',\n",
       "  'images': 146,\n",
       "  'conf': 0.3,\n",
       "  'iou_eval': 0.5,\n",
       "  'precision': 0.4134078212290503,\n",
       "  'recall': 0.6166666666666667,\n",
       "  'f1': 0.49498327759197325,\n",
       "  'mAP@50': 0.29181620478630066,\n",
       "  'mAP@50-95': 0.24289314448833466},\n",
       " {'run': 'GroundingDINO_RECALL',\n",
       "  'images': 146,\n",
       "  'conf': 0.3,\n",
       "  'iou_eval': 0.5,\n",
       "  'precision': 0.23665893271461716,\n",
       "  'recall': 0.85,\n",
       "  'f1': 0.3702359346642468,\n",
       "  'mAP@50': 0.3533213138580322,\n",
       "  'mAP@50-95': 0.2871873676776886},\n",
       " {'run': 'GroundingDINO_COMPROMISE',\n",
       "  'images': 146,\n",
       "  'conf': 0.3,\n",
       "  'iou_eval': 0.5,\n",
       "  'precision': 0.2383177570093458,\n",
       "  'recall': 0.85,\n",
       "  'f1': 0.37226277372262767,\n",
       "  'mAP@50': 0.3533213138580322,\n",
       "  'mAP@50-95': 0.2871873676776886},\n",
       " {'run': 'ENSEMBLE_CONSENSUS',\n",
       "  'images': 146,\n",
       "  'conf': 0.3,\n",
       "  'iou_eval': 0.5,\n",
       "  'precision': 0.819672131147541,\n",
       "  'recall': 0.4166666666666667,\n",
       "  'f1': 0.5524861878453039,\n",
       "  'mAP@50': 0.3652004599571228,\n",
       "  'mAP@50-95': 0.3366926908493042},\n",
       " {'run': 'ENSEMBLE_UNION',\n",
       "  'images': 146,\n",
       "  'conf': 0.3,\n",
       "  'iou_eval': 0.5,\n",
       "  'precision': 0.24018475750577367,\n",
       "  'recall': 0.8666666666666667,\n",
       "  'f1': 0.37613019891500904,\n",
       "  'mAP@50': 0.4037867784500122,\n",
       "  'mAP@50-95': 0.3457050323486328},\n",
       " {'run': 'ENSEMBLE_WBF',\n",
       "  'images': 146,\n",
       "  'conf': 0.3,\n",
       "  'iou_eval': 0.5,\n",
       "  'precision': 0.24018475750577367,\n",
       "  'recall': 0.8666666666666667,\n",
       "  'f1': 0.37613019891500904,\n",
       "  'mAP@50': 0.26860949397087097,\n",
       "  'mAP@50-95': 0.21282128989696503}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ==== АНСАМБЛИ: запуски ====\n",
    "if RUN_ENSEMBLE_CONSENSUS:\n",
    "    out_dir = os.path.join(os.path.dirname(PATH_IMAGES), \"ans_ensemble_consensus\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    results.append(evaluate_on_dataset(_consensus_ensemble, \"ENSEMBLE_CONSENSUS\", out_dir, save_images=True))\n",
    "    gpu_cleanup()\n",
    "\n",
    "if RUN_ENSEMBLE_UNION:\n",
    "    out_dir = os.path.join(os.path.dirname(PATH_IMAGES), \"ans_ensemble_union\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    results.append(evaluate_on_dataset(_union_ensemble, \"ENSEMBLE_UNION\", out_dir, save_images=True))\n",
    "    gpu_cleanup()\n",
    "\n",
    "if RUN_ENSEMBLE_WBF:\n",
    "    out_dir = os.path.join(os.path.dirname(PATH_IMAGES), \"ans_ensemble_wbf\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    results.append(evaluate_on_dataset(_wbf_ensemble, \"ENSEMBLE_WBF\", out_dir, save_images=True))\n",
    "    gpu_cleanup()\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e51dc46b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T17:10:49.354878Z",
     "iopub.status.busy": "2025-09-17T17:10:49.353870Z",
     "iopub.status.idle": "2025-09-17T17:10:49.376399Z",
     "shell.execute_reply": "2025-09-17T17:10:49.376399Z",
     "shell.execute_reply.started": "2025-09-17T17:10:49.354878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>mAP@50</th>\n",
       "      <th>mAP@50-95</th>\n",
       "      <th>images</th>\n",
       "      <th>conf</th>\n",
       "      <th>iou_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ENSEMBLE_CONSENSUS</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.552486</td>\n",
       "      <td>0.365200</td>\n",
       "      <td>0.336693</td>\n",
       "      <td>146</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OWLv2_COLOR</td>\n",
       "      <td>0.732394</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.544503</td>\n",
       "      <td>0.374188</td>\n",
       "      <td>0.344315</td>\n",
       "      <td>146</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GroundingDINO_PRECISE</td>\n",
       "      <td>0.413408</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.494983</td>\n",
       "      <td>0.291816</td>\n",
       "      <td>0.242893</td>\n",
       "      <td>146</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ENSEMBLE_UNION</td>\n",
       "      <td>0.240185</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.376130</td>\n",
       "      <td>0.403787</td>\n",
       "      <td>0.345705</td>\n",
       "      <td>146</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ENSEMBLE_WBF</td>\n",
       "      <td>0.240185</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.376130</td>\n",
       "      <td>0.268609</td>\n",
       "      <td>0.212821</td>\n",
       "      <td>146</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GroundingDINO_COMPROMISE</td>\n",
       "      <td>0.238318</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.372263</td>\n",
       "      <td>0.353321</td>\n",
       "      <td>0.287187</td>\n",
       "      <td>146</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GroundingDINO_RECALL</td>\n",
       "      <td>0.236659</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.370236</td>\n",
       "      <td>0.353321</td>\n",
       "      <td>0.287187</td>\n",
       "      <td>146</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GroundingDINO_BASE</td>\n",
       "      <td>0.139869</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.241808</td>\n",
       "      <td>0.236896</td>\n",
       "      <td>0.195611</td>\n",
       "      <td>146</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        run  precision    recall        f1    mAP@50  \\\n",
       "5        ENSEMBLE_CONSENSUS   0.819672  0.416667  0.552486  0.365200   \n",
       "0               OWLv2_COLOR   0.732394  0.433333  0.544503  0.374188   \n",
       "2     GroundingDINO_PRECISE   0.413408  0.616667  0.494983  0.291816   \n",
       "6            ENSEMBLE_UNION   0.240185  0.866667  0.376130  0.403787   \n",
       "7              ENSEMBLE_WBF   0.240185  0.866667  0.376130  0.268609   \n",
       "4  GroundingDINO_COMPROMISE   0.238318  0.850000  0.372263  0.353321   \n",
       "3      GroundingDINO_RECALL   0.236659  0.850000  0.370236  0.353321   \n",
       "1        GroundingDINO_BASE   0.139869  0.891667  0.241808  0.236896   \n",
       "\n",
       "   mAP@50-95  images  conf  iou_eval  \n",
       "5   0.336693     146   0.3       0.5  \n",
       "0   0.344315     146   0.3       0.5  \n",
       "2   0.242893     146   0.3       0.5  \n",
       "6   0.345705     146   0.3       0.5  \n",
       "7   0.212821     146   0.3       0.5  \n",
       "4   0.287187     146   0.3       0.5  \n",
       "3   0.287187     146   0.3       0.5  \n",
       "1   0.195611     146   0.3       0.5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary to: datasets/sirius/T-bank_val/val\\evaluation_summary_with_f1_and_ensembles.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== SUMMARY ====\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    # Сортируем по F1, затем по mAP@50-95\n",
    "    display(df[[\"run\",\"precision\",\"recall\",\"f1\",\"mAP@50\",\"mAP@50-95\",\"images\",\"conf\",\"iou_eval\"]]\n",
    "            .sort_values([\"f1\",\"mAP@50-95\",\"mAP@50\"], ascending=False))\n",
    "    out_csv = os.path.join(os.path.dirname(PATH_IMAGES), \"evaluation_summary_with_f1_and_ensembles.csv\")\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved summary to: {out_csv}\")\n",
    "else:\n",
    "    print(\"NO DATA\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
